# -*- python -*-
# This file is part of ci_middleware.
#
# Developed for the LSST Data Management System.
# This product includes software developed by the LSST Project
# (https://www.lsst.org).
# See the COPYRIGHT file at the top-level directory of this distribution
# for details of code ownership.
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program.  If not, see <https://www.gnu.org/licenses/>.
from __future__ import annotations

import os

from lsst.sconsUtils import state
from lsst.sconsUtils.utils import libraryLoaderEnvironment
from lsst.ci.middleware import (
    UNMOCKED_DATASET_TYPES,
    DEFAULTS_COLLECTION,
)


PIPELINES_DIR = os.path.join(os.environ["DRP_PIPE_DIR"], "pipelines", "HSC")
PIPETASK_BIN = os.path.join(os.environ["CTRL_MPEXEC_DIR"], "bin/pipetask")


def python_cmd(*args: str) -> str:
    """Return a command-line string that runs the Python executable.

    Parameters
    ----------
    *args
        Command-line arguments to pass to Python.

    Returns
    -------
    cmd : `str`
        A command-line string.
    """
    terms = [libraryLoaderEnvironment(), "python"]
    terms.extend(args)
    return " ".join(terms)


def tar_repo_cmd(input_dir: str, output_tar: str) -> str:
    """Return a command-line string that tars up a data repository.

    Parameters
    ----------
    input_dir : `str`
        Path to the data repository.  Will be deleted as the tar archive is
        created.
    output_tar : `str`
        Name of the output tar file.  Should reflect gzip compression.

    Returns
    -------
    cmd : `str`
        A command-line string.

    Notes
    -----
    This adds all data repository files and directories directly to the root of
    the tar archive, so when extracted it will turn the current directory (not
    some subdirectory of it) into a data repository.  This makes it easier to
    extract the data repository into a location that is different from the
    original one.
    """
    return f"tar --remove-files -czf {output_tar} -C {input_dir} ."


def untar_repo_cmd(source_tar: str, output_dir: str) -> str:
    """Return a command-line string that untars a data repository.

    Parameters
    ----------
    source_tar : `str`
        Path to the input tar file.  Should be gzip-compressed and hold data
        repository contents in the root of the archive, not some subdirectory
        (see `tar_repo_cmd`).
    output_dir : `str`
        Path to the output repository.

    Returns
    -------
    cmd : `str`
        A command-line string.
    """
    return f"rm -rf {output_dir} && mkdir {output_dir} && tar -C {output_dir} -xzf {source_tar}"


# Make a tarred-up repo with dimension records,skymap, and input datasets
# common to most mock pipelines (raws, calibs, refcats, etc.).
base_repo = state.env.Command(
    "base-repo.tgz",
    state.targets["version"],
    [
        python_cmd("-m", "lsst.ci.middleware", "make-base-repo", "--clobber", "${TARGET.base}"),
        tar_repo_cmd("${TARGET.base}", "${TARGET}"),
    ],
)
state.env.CleanTree(["base-repo.tgz"], ["base-repo"])


def make_pipeline_sources(input_repo: list, name: str) -> list:
    """Make a list of SCons source nodes for a command that depends on a
    tarred data repository and a pipeline.

    Parameters
    ----------
    input_repo : `list`
        Single-element list holding a data repository target produced by
        another SCons command.
    name : `str`
        Base name of an HSC pipeline in ``drp_pipe``, without the "DRP-" suffix
        or ".yaml" extension.

    Returns
    -------
    sources : `list`
        Two-element list containing the given data repository node and a new
        file node for the pipeline.
    """
    sources = input_repo.copy()
    assert len(sources) == 1
    sources.append(File(os.path.join(PIPELINES_DIR, f"DRP-{name}.yaml")))
    return sources


def build_inputs_repo(name: str) -> list:
    """Make a SCons target that creates a data repository with all
    overall-inputs for a mocked pipeline.

    This function first untars the ``base_repo`` and then adds the input
    datasets for this pipeline that are not present.

    Parameters
    ----------
    name : `str`
        Base name of an HSC pipeline in ``drp_pipe``, without the "DRP-" suffix
        or ".yaml" extension.

    Returns
    -------
    target : `list`
        Single-element kist of SCons target nodes representing the tarred-up
        data repository.
    """
    state.env.CleanTree([f"{name}-inputs.tgz"], [f"{name}-inputs"])
    return state.env.Command(
        f"{name}-inputs.tgz",
        make_pipeline_sources(base_repo, name),
        [
            untar_repo_cmd("${SOURCE}", "${TARGET.base}"),
            python_cmd(
                "-m lsst.ci.middleware prep-for-pipeline",
                "${TARGET.base}",  # data repository
                "${SOURCES[1]}",  # pipeline file path
            ),
            tar_repo_cmd("${TARGET.base}", "${TARGET}"),
        ],
    )


def build_pipetask_outputs(
    inputs: list,
    name: str,
    where: str = "",
    collection: str = "HSC/runs/{}",
    step: str | None = None,
    group: str | None = None,
) -> list:
    """Make a SCons target that runs the ``pipetask`` command on a pipeline.

    Parameters
    ----------
    input_repo : `list`
        Single-element list holding a data repository target produced by
        another SCons command.
    name : `str`
        Base name of an HSC pipeline in ``drp_pipe``, without the "DRP-" suffix
        or ".yaml" extension.
    where : `str`, optional
        Data ID query string, passed as the ``-d`` argument to ``pipetask``.
    collection : `str`, optional
        Output collection template, with a single `str.format` placeholder for
        the full name of this pipeline/step/group.
    step : `str`, optional
        Name of a pipeline step subset to run.
    group : `str`, optional
        Identifier for the data ID subset of the pipeline or pipeline step.

    Notes
    -----
    This function is intended to be used to build "baseline" versions of all
    QuantumGraphs and pipeline output datasets.  Alternate versions should
    generally be created by code in the ``tests`` directory, since regular unit
    tests are easier to maintain and parallelize than SCons targets, but repos
    created by test scripts cannot easily be used as inputs to other tests, in
    the way the SCons-target repos can be.
    """
    full_name = name if step is None else f"{name}-{step}"
    chain = collection.format(name)
    if step is not None:
        run = os.path.join(chain, step)
    else:
        run = os.path.join(chain, "run")
    if group is not None:
        run = os.path.join(run, group)
        full_name = f"{full_name}-{group}"
    # Targets are a tarred-up data repository, a quantum graph, and a log
    # file..  We'll use "${TARGETS[0].base}" as the name of the repo before
    # we tar it up.
    targets = [f"{full_name}.tgz", f"{full_name}.qgraph", f"{full_name}.log"]
    # I think SCons is supposed to know how clean targets automatically, but it
    # does not, and it can't be expected to clean up the temporary repo
    # directories anyway since it never knows about them.  Our sconsUtils
    # provides this function as a workaround, but sadly we can't pass the
    # $TARGET-based strings there.
    state.env.CleanTree(targets, [full_name])
    # Repo root will be expanded by scons based on the target we also pass to
    # scons.  This is better than doing it ourselves because it handles
    # absolute/relative paths consistently with the nodes SCons tracks the
    # dependencies of.
    repo_root = "${TARGETS[0].base}"
    return state.env.Command(
        # Force targets to be recognized as files.
        [File(t) for t in targets],
        make_pipeline_sources(inputs[:1], name),
        [
            # Untar the input data repository, which naturally makes a copy of
            # it, with the name we'll use for the output data repository.
            untar_repo_cmd("${SOURCE}", repo_root),
            # Run pipetask, telling it to save the QuantumGraph and log and run
            # the pipeline [step] in a single invocation.
            python_cmd(
                PIPETASK_BIN,
                "--long-log",
                "--log-file ${TARGETS[2]}",
                "--no-log-tty",
                "run",
                f"-b {repo_root}",
                "-p ${SOURCES[1]}" + ("" if step is None else f"#{step}"),
                f'-d "{where}"',
                f"--input {DEFAULTS_COLLECTION}",
                f"--output {chain}",
                f"--output-run {run}",
                "--save-qgraph ${TARGETS[1]}",
                "--qgraph-datastore-records",
                "--register-dataset-types",
                "--mock",
                f"--unmocked-dataset-types '{','.join(UNMOCKED_DATASET_TYPES)}'",
            ),
            # Tar up the output data repository.
            tar_repo_cmd(repo_root, "${TARGETS[0]}"),
        ],
    )


# ci_hsc is run as a full pipeline with no steps, with a constraint limiting
# it to a single patch.  This leads to "tract slicing", in which detectors
# that don't overlap that patch are dropped from steps that normally expect
# complete visits.  We don't care here because the most important tasks that
# care about tract slicing are not run in this pipeline, and for the remainder
# we just live with any slight degradation in the quality of the results.
ci_hsc_inputs = build_inputs_repo("ci_hsc")
ci_hsc = build_pipetask_outputs(ci_hsc_inputs, "ci_hsc", where="skymap='ci_mw' AND tract=3 AND patch=0")

# RC2 is run in steps, with the target of the real pipeline being outputs for
# just a few non-overlapping tracts.  We do care here about giving full visits
# to steps that expect full visits (no "tract slicing"), and that's the main
# reason for splitting up into steps.  For the mock version, we process just
# one tract (0), and deliberately leave out the one visit here (96980) that
# doesn't overlap it.  Some patches in this tract will not have any data in one
# or both bands.
rc2_inputs = build_inputs_repo("RC2")
rc2_step1 = build_pipetask_outputs(
    rc2_inputs, "RC2", where="instrument='HSC' AND exposure != 96980", step="step1"
)
rc2_step2a = build_pipetask_outputs(rc2_step1, "RC2", where="skymap='ci_mw'", step="step2a")
rc2_step2b = build_pipetask_outputs(rc2_step2a, "RC2", where="skymap='ci_mw' AND tract=0", step="step2b")
rc2_step2cde = build_pipetask_outputs(rc2_step2b, "RC2", step="step2cde")
rc2_step3 = build_pipetask_outputs(rc2_step2cde, "RC2", where="skymap='ci_mw' AND tract=0", step="step3")
rc2_step4 = build_pipetask_outputs(rc2_step3, "RC2", step="step4")
rc2_step5 = build_pipetask_outputs(rc2_step4, "RC2", where="skymap='ci_mw' AND tract=0", step="step5")
rc2_step6 = build_pipetask_outputs(rc2_step5, "RC2", step="step6")
rc2_step7 = build_pipetask_outputs(rc2_step6, "RC2", step="step7")
rc2_step8 = build_pipetask_outputs(rc2_step7, "RC2", where="", step="step8")
rc2 = rc2_step8

# The Prod pipeline is run in steps on very large amounts of data, in which we
# expect each step to have to be split up into multiple submissions to keep QG
# and workflow size under control.  We process all data in the input
# repository, and split up steps into groups where appropriate.  We process
# independent groups in serial - which is not realistic, but a useful
# simplification - in order to keep up the practice of making a new repo for
# each run, with no need to merge them.
prod_inputs = build_inputs_repo("Prod")
# Using band for grouping here is just a convenient to split our visits up
# into two groups, not a reflection of expected real pipeline usage.
prod_step1_r = build_pipetask_outputs(prod_inputs, "Prod", where="band='r'", step="step1", group="r")
prod_step1_i = build_pipetask_outputs(prod_step1_r, "Prod", where="band='i'", step="step1", group="i")
prod_step2a_r = build_pipetask_outputs(prod_step1_i, "Prod", where="band='r'", step="step2a", group="r")
prod_step2a_i = build_pipetask_outputs(prod_step2a_r, "Prod", where="band='i'", step="step2a", group="i")
prod_step2b_even = build_pipetask_outputs(
    prod_step2a_i, "Prod", where="skymap='ci_mw' AND tract IN (0, 2)", step="step2b", group="even"
)
prod_step2b_odd = build_pipetask_outputs(
    prod_step2b_even, "Prod", where="skymap='ci_mw' AND tract IN (1, 3)", step="step2b", group="odd"
)
prod_step2c = build_pipetask_outputs(prod_step2b_odd, "Prod", step="step2c")
prod_step2d_r = build_pipetask_outputs(prod_step2c, "Prod", where="band='r'", step="step2d", group="r")
prod_step2d_i = build_pipetask_outputs(prod_step2d_r, "Prod", where="band='i'", step="step2d", group="i")
prod_step2e = build_pipetask_outputs(prod_step2d_i, "Prod", step="step2d")
prod_step3_even = build_pipetask_outputs(
    prod_step2e, "Prod", where="skymap='ci_mw' AND tract IN (0, 2)", step="step3", group="even"
)
prod_step3_odd = build_pipetask_outputs(
    prod_step3_even, "Prod", where="skymap='ci_mw' AND tract IN (1, 3)", step="step3", group="odd"
)
prod_step4_r = build_pipetask_outputs(prod_step3_odd, "Prod", where="band='r'", step="step4", group="r")
prod_step4_i = build_pipetask_outputs(prod_step4_r, "Prod", where="band='i'", step="step4", group="i")
prod_step7 = build_pipetask_outputs(prod_step4_i, "Prod", step="step7")
# step8 in DRP-Prod is not actually valid, because it has tasks that should be
# partitioned in different ways in order to scale up.  This is DM-39314.
prod = prod_step7
